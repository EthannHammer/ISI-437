{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión\n",
    "En este notebook exploraremos los conceptos básicos e implementación de la regresión lineal, para ajuste de curvas y la regresión logística para clasificación binaria.\n",
    "\n",
    "## Regresión Lineal\n",
    "Como ya se exploró anteriormente, podemos definir la regresión lineal como un ajuste de curvas en el cual nuestra hipótesis es una combinación lineal de las entradas y los parámetros.\n",
    "\n",
    "### Hipótesis\n",
    "Sea $\\mathbf{x}$ un vector de entrada con $m$ elementos donde cada elemento se denota por $x_1, x_2, \\dots x_m$, el vector $\\mathbf{x}$ se corresponde con un vector de salida $\\mathbf{y}$ con la misma cantidad de elementos. Cada elemento $x_i$ cuenta con su correspondiente valor de salida $y_i$. Sea $\\mathbf{w}$ un vector que representa a los coeficientes o parámetros de la regresión lineal, y un elemento $b$ que se denomina el *bias* estos parámetros definen la naturaleza del hiperplano que representa a la solución. La función de hipótesis para la regresión lineal se define de la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "h(\\mathbf{x}, \\mathbf{w}, b) = w_1 x_1 + w_2 x_2 \\dots w_m x_m + b\n",
    "\\end{equation}\n",
    "\n",
    "### Función de costo\n",
    "La función de costo está definida por el **error cuadrático medio** o MSE denotada por la siguiente ecuación:\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^m(h(x_i)-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Descenso de gradiente\n",
    "La forma de encontrar la solución con error mínimo se denomina **descenso de gradiente** y está definido de la siguiente forma:\n",
    "\\begin{equation}\n",
    "Repetir: \\\\\n",
    "    \\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial{J}}{\\partial{\\mathbf{w}}}\n",
    "\\end{equation}\n",
    "\n",
    "$\\alpha$ es el *learning rate*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = X.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [random.random() for x in X[0]]\n",
    "b = random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x,w,b):\n",
    "    res = 0\n",
    "    for i in range(len(x)):\n",
    "        res += w[i]*x[i]\n",
    "    res +=b\n",
    "    return res\n",
    "def cost(x, y, hyp, w, b):\n",
    "    m = len(x)\n",
    "    res = 0\n",
    "    for i in range(len(x)):\n",
    "        res += (hyp(x[i], w, b) - y[i])**2\n",
    "    return res/(2*m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_cost_w(x, y, hyp, w, b):\n",
    "    # w\n",
    "    m = len(x)\n",
    "    deriv = []\n",
    "    for j in range(len(w)):\n",
    "        res = 0\n",
    "        for i in range(len(x)):\n",
    "            res += (hyp(x[i],w,b) - y[i])*x[i][j]\n",
    "        res /= m\n",
    "        \n",
    "        deriv.append(res)\n",
    "    return deriv\n",
    "        \n",
    "def d_cost_b(x, y, hyp, w, b):\n",
    "    m = len(x)\n",
    "    res = 0\n",
    "    for i in range(len(x)):\n",
    "        res += (hyp(x[i],w,b) - y[i])\n",
    "    res /= m\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "w = [random.random() for x in X[0]]\n",
    "b = random.random()\n",
    "# entrenamiento\n",
    "# repetir hasta la convergencia\n",
    "alpha = 0.0001\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    for j in range(len(w)):\n",
    "        w[j] = w[j] - alpha * d_cost_w(X,y,h,w,b)[j]\n",
    "    b = b - alpha * d_cost_b(X,y,h,w,b)\n",
    "    print(cost(X,y,h,w,b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizacion de vectores de entrada *extra*\n",
    "Los pasos para normalizar las entradas:\n",
    "1. Encontrar los rangos (min, max) de cada una de las columnas del vector\n",
    "2. Generar una funcion (lista, operacion) que convierta un vector a su forma normalizada\n",
    "3. Entrenar el algoritmo con esos vectores normalizados\n",
    "\n",
    "Si la normalización esta bien implementada, el entrenamiento debería realizarse correctamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
